from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import time

prompt = ChatPromptTemplate.from_template(
    "æ¬¡ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„: {topic}"
)
model = ChatOllama(model="llama3.2:1b", temperature=0.7)
output_parser = StrOutputParser()

chain = prompt | model | output_parser
print("=== é€šå¸¸ã®å¿œç­” ===")
start = time.time()
result = chain.invoke({"topic": "æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤"})
end = time.time()

print(f"å¿œç­”ï¼ˆ{end - start:.2f}ç§’å¾Œï¼‰:")
print(result)

print("\n=== ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­” ===")
start = time.time()

for chunk in chain.stream({"topic": "äººå·¥çŸ¥èƒ½ã®æ­´å²"}):
    print(chunk, end="", flush=True)

end = time.time()
print(f"\n(å®Œäº†: {end - start:.2f}ç§’)")

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"),
    ("user", "{question}")
])

chat_chain = chat_prompt | model | output_parser

questions = [
    "Pythonã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "ãªãœPythonã¯äººæ°—ãŒã‚ã‚‹ã®ã§ã™ã‹ï¼Ÿ"
]

for question in questions:
    print(f"\nğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼: {question}")
    print("ğŸ¤– AI: ", end="", flush=True)
    
    for chunk in chat_chain.stream({"question": question}):
        print(chunk, end="", flush=True)
    
    print()  # æ”¹è¡Œ
